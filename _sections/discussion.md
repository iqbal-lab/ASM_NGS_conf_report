---
title: "Discussion"
order: 4
---

It has become a truism in genomics that to study a sample, you must either work wholly _de novo_, or do "reference-assisted assembly", which generally means map-to-a-reference. Working wholly _de novo_ is unbiased, but generally falls apart when samples are not clonal. Mapping to a single reference creates problems when your sample is not closely related to the reference. What we would like to do with Cortex and Outbryk is free ourselves of this false dichotomy. Our approach is to look at our samples in the context of the whole species, use an arbitrary reference purely for clustering samples, and then look for variation between reasonably closely related samples _without any reference present to bias variant discovery_. Only after that do we use a new cluster-specific reference to provide a coordinate system. We've been trialling MASH, from the Phillippy lab, as a tool to enable that choice of local reference, but in the case of this data, MASH chose the reference genome we were going to use anyway, so we'll be testing this in more detail in future. By doing all of this, we not only get state of the art quality SNP and indel calls, but we increase our sensitivity. By doing multi-sample variant assembly we are also able to assay the accessory genome either via querying for presence of curated sequence (in our case phages) or in a hypothesis free way, simply looking at contig sharing across samples. Even in the relatively small datasets for this challenge, the accessory genome provided interesting and complementary information to the SNP/indel calls.

Each Cortex genotype call comes with a confidence, which is the log likelihood of the data given the called genotype minus the log likelihood of the next most likely genotype. Typically users can use this to calibrate their callset location on the sensitivity-specificity curve. However Cortex stops once you have your variants in a VCF file. This ASM NGS challenge specifically wanted to know what pipelines did to find clusters and to rule samples in/out of being in an outbreak. Our new pipeline therefore build on Cortex and takes you some way forward, although not all the way to a statistical outbreak prediction.

 Outbryk is still currently in the proof of concept stage, and does not make statements about whether a given cluster is an outbreak or not. Outbryk is not yet to be considered alpha software, so users trial it at their own risk, although any user would be viewed with great affection by us. Cortex on the other hand is very robust, and you may choose to integrate it directly into your workflows. It is now hosted on [github](https://github.com/iqbal-lab/cortex), a fact which we have not broadcast while we heavily tested the new workflows, which are designed to scale massively, and to make it easiser for users to run. 

 Having had the chance to see the presentations from all the other competitors, several things are clear. Firstly, there are a host of alternatives out there, based  on either some type of MLST or whole-genome approaches or both, and we were impressed with the systematic QC that was applied in many places, the integration with NCBI in order to compare with a broader set of background data, and some of the user-interfaces to enable exploration of the data. Aspects of the performance of Outbryk have exceeded our expectations: the ability to churn through thousands of samples very rapidly, and combine both SNP/indel and accessory analysis is we think very valuable. The zoom-and-analyse-clusters approach, using an appropriate reference will be a huge benefit, especially in species where we do not have well curated MLST-to-reference-choice solutions. However Outbryk does need more work, and we were not satisfied with the fact that some lower quality SNPs got through to our trees. This has been a tremendous challenge to work on, and we would like to thank the organisers for organising it so well.

